{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from Scratch\n",
    "\n",
    "Binary classifier that implements logistic regression. Notebook content is based off of lessons from Coursera Neural Networks and Deep Learning course.\n",
    "\n",
    "\n",
    "**Note**: Logistic regression is _very easy_ to do in sklearn, but the process of creating this notebook was to develop a foundation for creating neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Functions\n",
    "\n",
    "- `sigmoid()`: Activation function, converts weighted average of features to value between 0 and 1\n",
    "    * Input\n",
    "        - `z`: weighted average of features (i.e., w*X + b)\n",
    "    * Output\n",
    "\n",
    "\n",
    "- `initialize_with_zeros()`\n",
    "    * Input\n",
    "        - `dim`: number of features represented by data\n",
    "    * Output\n",
    "        - `w`: vector of zeros to represent relative weight of each feature (*n_features x 1*)\n",
    "        - `b`: 0, to represent bias\n",
    "        \n",
    "        \n",
    "- `propagate()`: forward propagation (compute cost associated with parameters), backward propagation (compute gradient based on error)\n",
    "    * Input\n",
    "        - `w`: vector representing weight of each feature (*n_features x 1*)\n",
    "        - `b`: scalar representing bias\n",
    "        - `X`: collection of features for each data sample (*n_features x n_samples*)\n",
    "        - `Y`: label (1 or 0) for each data sample (*1 x n_samples*)\n",
    "    * Output\n",
    "        - `grads`: dictionary of gradients assigned to parameters to reduce cost\n",
    "            * `grads[\"dw\"]`: vector of gradients to apply to w (*n_features x 1*)\n",
    "            * `grads[\"db\"]`: scalar gradient to apply to b\n",
    "        - `cost`: cost of current parameters\n",
    "\n",
    "\n",
    "- `optimize()`: optimizes w and b through gradient descent\n",
    "    * Input\n",
    "        - `w`: vector representing weight of each feature (*n_features x 1*)\n",
    "        - `b`: scalar representing bias\n",
    "        - `X`: collection of features for each data sample (*n_features x n_samples*)\n",
    "        - `Y`: label (1 or 0) for each data sample (*1 x n_samples*)\n",
    "        - `num_interations`: number of iterations of the optimization loop\n",
    "        - `learning_rate`: learning rate of the gradient descent update rule\n",
    "        - `print_cost`: flag to print loss every 100 iterations (default False)\n",
    "    * Output\n",
    "        - `params`: dictionary containing weights w and bias b\n",
    "        - `grads`: dictionary containing gradients (dw, db) with respect to cost function\n",
    "        - `costs`: list of all costs computed during optimization, to plot learning curve\n",
    "\n",
    "\n",
    "- `predict()`: predicts class for test data\n",
    "    * Input\n",
    "        - `w`: vector representing weight of each feature (*n_features x 1*)\n",
    "        - `b`: scalar representing bias\n",
    "        - `X`: collection of features for each data sample (*n_features x n_samples*)\n",
    "    * Output\n",
    "        - `Y_prediction`: class prediction (0 or 1) for each sample in X (*1 x n_samples*)\n",
    "\n",
    "\n",
    "- `model()`: fits model and returns fit information\n",
    "    * Input\n",
    "        - `X_train`: collection of features for each data sample (*n_features x n_train_samples*)\n",
    "        - `Y_train`: label (1 or 0) for each sample in X_train (*1 x n_train_samples*)\n",
    "        - `X_test`: collection of features for each data sample (*n_features x n_test_samples*)\n",
    "        - `Y_test`: label (1 or 0) for each sample in X_train (*1 x n_test_samples*)\n",
    "        - `num_iterations`: number of iterations of the optimization loop (default 2000)\n",
    "        - `learning_rate`: learning rate of the gradient descent update rule (default 0.5)\n",
    "        - `print_cost`: flag to print loss every 100 iterations (default False)\n",
    "    * Output\n",
    "        - `d`: dictionary of model fit information\n",
    "            * `d[\"costs\"]`: cost of model parameters every 100 iterations\n",
    "            * `d[\"Y_prediction_train\"]`: predicted classes for training data\n",
    "            * `d[\"Y_prediction_test\"]`: predicted classes for testing data\n",
    "            * `d[\"w\"]`: fitted weights for each feature\n",
    "            * `d[\"b\"]`: fitted bias term for model\n",
    "            * `d[\"learning_rate\"]`: value from input\n",
    "            * `d[\"num_iterations\"]`: value from input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dictionaries\n",
    "import numpy as np\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of z\n",
    "\n",
    "    Input:\n",
    "        z: A scalar or numpy array of any size.\n",
    "    Output:\n",
    "        s: sigmoid(z)\n",
    "    \"\"\"    \n",
    "    s = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize w and b with zeros\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    Create a vector of zeros of shape (dim, 1) for w and initialize b to 0.\n",
    "    \n",
    "    Input:\n",
    "        dim: number of features in data\n",
    "    Output:\n",
    "        w: initialized vector representing feature weights (dim x 1)\n",
    "        b: initialized scalar representing bias\n",
    "    \"\"\"\n",
    "\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement cost function for parameter values and compute gradient based on error\n",
    "\n",
    "    Input:\n",
    "        w: weights, numpy array (n_features x 1)\n",
    "        b: bias, scalar\n",
    "        X: data of size (n_features x n_samples)\n",
    "        Y: true \"label\" vector (0, 1) of size (1 x n_samples)\n",
    "    Output:\n",
    "        cost: negative log-likelihood cost for logistic regression\n",
    "        grads: dictionary containing gradients for w and b\n",
    "            grads[\"dw\"]: gradient of the loss with respect to w (n_features x 1)\n",
    "            grads[\"db\"]: gradient of the loss with respect to b (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "\n",
    "    # Forward propagation: compute cost from X and parameters\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(Z) # activation\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1-Y)*np.log(1-A)) # cost\n",
    "\n",
    "    # Backward propagation: compute gradients for w and b\n",
    "    dZ = A - Y\n",
    "    dw = 1/m * np.dot(X, dZ.T)\n",
    "    db = 1/m * np.sum(dZ)\n",
    "\n",
    "    # Convert cost to scalar\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(not np.isnan(cost))# If cost is nan, possible that sigmoid returned 1s, meaning features need scaling\n",
    "\n",
    "    # Store gradients in dictionary\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent to optimize parameters\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize w and b\n",
    "    \n",
    "    Input:\n",
    "        w: weights, numpy array (n_features x 1)\n",
    "        b: bias, scalar\n",
    "        X: data of size (n_features x n_samples)\n",
    "        Y: true \"label\" vector (0, 1) of size (1 x n_samples)\n",
    "        num_iterations: number of iterations of the optimization loop\n",
    "        learning_rate: learning rate of the gradient descent update rule\n",
    "        print_cost: True to print the loss every 100 steps\n",
    "    Output:\n",
    "        params: dictionary containing the weights w and bias b\n",
    "        grads: dictionary containing the gradients of the weights (dw) and bias (db) with respect to the cost function\n",
    "        costs: list of all the costs computed during the optimization (length: floor(n_samples/100)).\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Cost and gradient calculation\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "    Predict whether label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Input:\n",
    "        w: weights, numpy array (n_features x 1)\n",
    "        b: bias, scalar\n",
    "        X: data of size (n_features x n_samples)\n",
    "    Output:\n",
    "        Y_prediction: numpy array (vector) containing predictions (0/1) for the examples in X (length: n_samples)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1] # number of features\n",
    "    Y_prediction = np.zeros((1,m)) # initialize vector for Y_prediction\n",
    "\n",
    "    # Compute vector A predicting the probabilities of a cat being present in the picture\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A[0,i] > 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Build logistic regression model\n",
    "    \n",
    "    Input:\n",
    "        X_train: collection of features for each data sample (n_features x n_train_samples)\n",
    "        Y_train: label (1 or 0) for each sample in X_train (1 x n_train_samples)\n",
    "        X_test: collection of features for each data sample (n_features x n_test_samples)\n",
    "        Y_test: label (1 or 0) for each sample in X_train (1 x n_test_samples)\n",
    "        num_iterations: number of iterations of the optimization loop (default 2000)\n",
    "        learning_rate: learning rate of the gradient descent update rule (default 0.5)\n",
    "        print_cost: flag to print loss every 100 iterations (default False)\n",
    "    Output:\n",
    "        d: dictionary containing information about the model\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize parameters with zeros\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    # Retrieve parameters w and b from dictionary\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "\n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test errors\n",
    "    print(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"Test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    # Store model summary in dictionary d\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test,\n",
    "         \"Y_prediction_train\" : Y_prediction_train,\n",
    "         \"w\" : w,\n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Test model\n",
    "Load breast cancer dataset and test model on classifying tumors as malignant (0) or benign (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 98.12206572769954 %\n",
      "Test accuracy: 96.5034965034965 %\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "# Read x and y data\n",
    "X = data['data']\n",
    "Y = data['target']\n",
    "target_names = data['target_names']\n",
    "\n",
    "# Split into training and testing trials\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y)\n",
    "\n",
    "# Min-max scaling of features\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data so that it's in the format we want\n",
    "#     X: n_features x n_trials\n",
    "#     Y: 1 x n_trials\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "Y_train = Y_train.reshape(1,-1)\n",
    "Y_test = Y_test.reshape(1,-1)\n",
    "\n",
    "# Fit model on data\n",
    "d = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create plot of cost vs. iteration number\n",
    "\n",
    "# x_plot = iteration number; y_plot = cost\n",
    "x_plot = np.arange(0, d[\"num_iterations\"], 100)\n",
    "y_plot = d[\"costs\"]\n",
    "\n",
    "# Generate plot\n",
    "plt.figure()\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.title('Cost of logistic regression model by iteration number')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
